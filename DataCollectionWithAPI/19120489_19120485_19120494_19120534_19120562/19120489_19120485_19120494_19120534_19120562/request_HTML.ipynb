{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='green'>ĐỒ ÁN 1: THU THẬP DỮ LIỆU\n",
    "## Danh sách các thành viên và bảng phân công công việc:\n",
    "| STT | Họ và tên | MSSV | Nội dung công việc | Hoàn thành |\n",
    "| :--- | :---: | :---: | :---: | ---: |\n",
    "| 01 | Lưu Trường Dương | 19120489 |Đọc và xử lý dữ liệu playlists phần parse HTML, xử lý bỏ qua lỗi của web driver và click accept | 100%|\n",
    "| 02 | Nguyễn Phạm Quang Dũng | 19120485 | Đọc và xử lý dữ liệu tracks phần parse HTML, xử lý link của các track khi thu được, tối ưu thời gian chờ load web | 100% |\n",
    "| 03 | Huỳnh Quốc Duy | 19120494 | Đọc và xử lý dữ liệu users phần parse HTML, chỉnh sửa lại các file sau khi đã Crawl xuống | 100% |\n",
    "| 04 | Phạm Đức Huy | 19120534 | Đọc và xử lý dữ liệu playlists & tracks phần API, xử lý trackIds | 100% |\n",
    "| 05 | Lê Thành Lộc | 19120562 | Đọc và xử lý dữ liệu users phần API & làm markdown, tự động tìm client_id | 100% |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import các package cần thiết"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import requests_cache\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime,timedelta\n",
    "from selenium import webdriver\n",
    "from requests_html import HTML\n",
    "import lxml\n",
    "import urllib.robotparser\n",
    "import csv\n",
    "#wait\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kiểm tra xem website có cho thu thập dữ liệu không? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check robots.txt\n",
    "def check_robots(url):    \n",
    "    rp = urllib.robotparser.RobotFileParser()\n",
    "    rp.set_url(url)\n",
    "    rp.read()\n",
    "    check  = rp.can_fetch(\"*\",url)\n",
    "    if not check:\n",
    "        print(\"Khong the get url tu trang web nay\")\n",
    "        exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup các biến toàn cục"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#luu link cac playlist\n",
    "link_playlist = []\n",
    "#dung de luu link cac track\n",
    "link_alltrack = []\n",
    "# dung de luu link cac user\n",
    "link_alluser =[] \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quá trình chạy Web Driver\n",
    "- Dùng webdriver của module selenium để chạy những trang web yêu cầu phải chạy javascript mới cho lấy dữ liệu từ biến\n",
    "#### Quá trình chạy webdriver trải qua các bước sau:\n",
    "   - Kiểm tra xem trang web có cho phép thu thập dữ liệu hay không với `robots.txt`\n",
    "   - Bỏ qua thông báo các lỗi `ssl-error-code-1` (các lỗi trong khung hồng phía dưới)\n",
    "   - Dùng WebDriverWait của selenium để đợi một thời gian ngắn ( ở đây là 20 giây ) để trang web load xong (khi tìm thấy id = 'onetrust-accept-btn-handler') , nếu quá thời gian chờ thì sẽ thông báo bị TimeOut và thoát webdriver\n",
    "   - Click button `\"Accept all cookies\"` để có thể bắt đầu thu thập dữ liệu (để có thể lấy giá trị các biến từ tag script của trang web) bằng cách tìm ô chứa id = onetrust-accept-btn-handler và click vào đó\n",
    "   \n",
    "#### Các biến quan trọng:\n",
    "\n",
    "   - `url`: link đi tới trang web [SoundCloud](https://soundcloud.com/)\n",
    "   - `key_word`: danh sách các thể loại nhạc cho biết muốn tìm các playlist với các từ khóa nào"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chay driverWeb\n",
    "url = \"https://soundcloud.com\"\n",
    "\n",
    "key_word = ['indie','pop','country','lofi','rock','rap','edm','baroque','piano','jazz','electric','chill']\n",
    "# check_robots(url+'/search/sets?q=indie')\n",
    "\n",
    "#bo qua thong bao cac loi ssl-error-code-1\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"--ignore-certificate-error\")\n",
    "options.add_argument(\"--ignore-ssl-errors\")\n",
    "driver = webdriver.Chrome(executable_path='chromedriver.exe')\n",
    "driver.get(url)\n",
    "\n",
    "#doi trang load xong bang cach tim id = \"onetrust-accept-btn-handler\"\n",
    "try:\n",
    "    main =  WebDriverWait(driver,20).until(\n",
    "        EC.presence_of_element_located((By.XPATH,'//*[@id=\"onetrust-accept-btn-handler\"]'))\n",
    "    )\n",
    "except:\n",
    "    print('TimeOut')\n",
    "    driver.quit()\n",
    "#click accept\n",
    "accept = driver.find_element_by_id('onetrust-accept-btn-handler').click()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thu thập danh sách các link playlist \n",
    "#### Quá trình thu thập danh sách link playlist\n",
    "   - Duyệt qua từng thể loại nhạc trong `key_word` để bắt đầu quá trình thu thập.\n",
    "   - Vì muốn tìm dữ liệu bắt đầu từ playlist nên ta sẽ duyệt thằng vào đường link tải các playlist có tên từ khoá trong key_word. Đường link sẽ là một string được ghép bởi biến toàn cục `url` và f-string `f'/search/sets?q={key_word[i]}'` \n",
    "   - Dùng WebDriverWait để đợi load xong trang web (dấu hiệu nhận biết là tìm thấy class tên là `searchList__item`. Đây là class xuất hiện trong `searchList`, cửa sổ thường được load sau cùng)\n",
    "   - Để tăng tối đa tốc độ cuộn của trang web thì ở đây ta sẽ dùng vòng lặp while chạy liên tục và tính thời gian bằng time.time(). Ở đây ta sẽ scroll 15 lần và mỗi lần scroll di chuyển một đoạn `document.body.scrollHeight`. Trong quá trình thì ta sẽ cộng biến run_time với khoảng thời gian từ khi bắt đầu vòng while cho tời tời điểm so sánh, cứ như vậy nếu scroll thì sẽ trả về lại run_time = 0 còn không thì sẽ diễn ra TimeOut khi run_time > max_run_time. Ngoài ra với mỗi lần scroll thành công thì biến `flag` sẽ tự cộng thêm 1 đơn vị.\n",
    "   - Dừng `5(s)` để trang web load xong và thu thập toàn bộ các link playlist mà web vừa load được sau quá trình scroll\n",
    "   - Dùng CSS Selector để tìm các link playlist từ trang web mà ta vừa load qua attrs `href`\n",
    "   - extend link vừa thu được vào list toàn cục link_playlist. Dùng pandas.unique để lọc những link bị trùng với nhau. Nếu số lượng link >1000 thì dừng quá trình get lại."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dung de lay link_playlist\n",
    "for i in range(0,len(key_word)):\n",
    "    driver.get(url+ f'/search/sets?q={key_word[i]}')\n",
    "    #xu ly thoi gian doi webload\n",
    "    try:\n",
    "        main =  WebDriverWait(driver,10).until(\n",
    "            EC.presence_of_element_located((By.CLASS_NAME,\"searchList__item\"))\n",
    "        )\n",
    "    except:\n",
    "        continue\n",
    "   \n",
    "    #scroll\n",
    "    pre_scroll_height = driver.execute_script('return document.body.scrollHeight;')\n",
    "    run_time= 0\n",
    "    max_run_time =10\n",
    "    flag = 0\n",
    "\n",
    "    while flag<15:\n",
    "        iteration_start = time.time()\n",
    "     \n",
    "        driver.execute_script('window.scrollTo(0, document.body.scrollHeight);')\n",
    "\n",
    "        post_scroll_height = driver.execute_script('return document.body.scrollHeight;')\n",
    "\n",
    "        scrolled = post_scroll_height != pre_scroll_height\n",
    "        timed_out = run_time >= max_run_time\n",
    "\n",
    "        if scrolled:\n",
    "            run_time = 0\n",
    "            pre_scroll_height = post_scroll_height\n",
    "            flag +=1\n",
    "        elif not scrolled and not timed_out:\n",
    "            run_time += time.time() - iteration_start\n",
    "        elif not scrolled and timed_out:\n",
    "            break  \n",
    "    #dung 5 giay de load het trang\n",
    "    time.sleep(5)\n",
    "    soup  = bs(driver.page_source, 'lxml')\n",
    "    temp  =  soup.select('div.soundTitle__usernameTitleContainer a.sc-link-primary.soundTitle__title.sc-link-dark.sc-text-h4')\n",
    "    list_search = [link['href'] for link in temp]\n",
    "    link_playlist.extend(list_search)\n",
    "    link_playlist = pd.unique(link_playlist).tolist()\n",
    "    if len(link_playlist) > 1000:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lưu danh sách các link playlist vừa thu thập được vào file `link_playlist.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save all link_playlist get to link_playlist.txt\n",
    "writer = open('link_playlist.txt','wt',encoding=\"utf-8\")\n",
    "for i in link_playlist:\n",
    "    writer.write(i)\n",
    "    writer.write('\\n')\n",
    "writer.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Đọc danh sách các link playlist từ `link_playlist.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read link_playlist from link_playlist.txt\n",
    "link_playlist.clear()\n",
    "reader = open('link_playlist.txt','r')\n",
    "for i in reader:\n",
    "    link_playlist.append(i.strip('\\n'))\n",
    "reader.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tạo một hàm `convert_track_link`\n",
    "- Để xử lý link track trong quá trình get. Vì `link trang khi ghép được`  = `link track gốc` + `thông tin playlist chứa nó`. Mục đích để có thể sử dụng sau này nếu như playlist không còn chứa track này nữa, và để xử lý việc trùng lặp link (do 1 track có thể nằm tại nhiều playlist khác nhau nên 1 track có thể có nhiều link) cũng như để link được gọn và đẹp hơn khi thêm vào file csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Xu ly link_alltrack track thu duoc tu playlist de cho duong link dep hon\n",
    "def convert_track_link(track_link):\n",
    "    temp = track_link.split('/')\n",
    "    temp_1 = temp[2].split('?')[0]\n",
    "    result = f'/{temp[1]}/{temp_1}'\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract các links playlist bằng parse HTML và lưu thông tin vào `playlist_temp.csv`\n",
    "### Lưu ý:\n",
    "   - Đặt tên là `playlist_temp.csv` vì cần phải xử lý mới ra được file playlist.csv đúng với yêu cầu đề bài\n",
    "   - Dù đã lưu các link vào file nhưng có thể trong quá trình get link, do user thay đổi đường link cá nhân của họ dẫn đến link bị sai do chưa cập nhật lại\n",
    "   - Do có một số track không cho phát tại Việt Nam nên trong quá trình, có thể số track > 4 nhưng chỉ lấy về 0 hoặc 1 track của playlist\n",
    "### File `playlist_temp.csv` gồm các cột:\n",
    "   - `ID`: số thứ tự của playplist nhưng vì không get được id nên dùng làm id luôn\n",
    "   - `Name`: tên của playlist \n",
    "   - `Link`: đường link tới playlist\n",
    "   - `UserName`: tác giả của playlist\n",
    "   - `Type`: Thể loại playlist, ở trên web được gọi là tag, trả về None nếu không có\n",
    "   - `NumberTrack`: số track trong playlist\n",
    "   - `Duration`: tổng thời gian cần để phát hết playlist \n",
    "   - `DateCreate`: thời gian tạo playlist\n",
    "   - `Like`: số lượt like của playlist\n",
    "   - `Repost`: số lượng đăng lại của playlist\n",
    "   - `TrackIds`: là một string chứa danh sách id của các track trong playlist (Hiện tại chỉ chứa string là các link track, sau khi xử lý ở giai đoạn sau crawl mới thành id)\n",
    "   \n",
    "### Xử lý:\n",
    "   - Mỗi lần get thì dùng `WebDriverWait` để đợi, nếu TimeOut thì sẽ chuyển qua get link tiếp theo.\n",
    "   - Những thông tin riêng lẽ thì có thể select riêng.\n",
    "   - Dùng `trackSummary` để có thể select cùng lúc `playlist_numbertrack` và `playlist_duration`\n",
    "   - Dùng `playlist_miniStats` để select cùng lúc `like` `repost`\n",
    "   - Thu thập danh sách các `link_user` và lưu vào `link_alluser` sau đó lọc các users bị trùng lặp\n",
    "   - Thu thập danh sách các `link_track` và lưu vào `link_alltrack` sau đó lọc các tracks bị trùng lặp\n",
    "       - Chuyển `list_track` thành string rồi lưu nó vào trackIds\n",
    "   - Cả `users` và `tracks` đều chỉ lấy 4 phần tử đầu do sợ số lượng quá nhiều, thời gian crawl lâu.\n",
    "   - Do có một số playlist để ẩn track ( private) nên trong quá trình get vẫn lấy được `duration`, nên những trường hợp này trả `duration` về 0 luôn\n",
    "### Phần code được comment lại ở chỗ xử lý playlist\n",
    "   - Nếu tốc độ mạng được đảm bảo ổn định thì có thể dùng đoạn code comment để có thể trực tiếp tìm ra các list ID và trực tiếp lưu vào `trackIds` luôn mà không cần phải xử lý dữ liệu sau khi Crawl về nữa. Bằng cách tìm index phần tử của list vừa thu thập được trong `list_alltrack` ( do `list_alltrack` được cập nhật từ các `link_track`)\n",
    "   - Nhưng để phòng trường hợp quá trình get bị lỗi thì nên để sau khi crawl về rồi xử lý data sau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#lay info playlist va luu vao playlist.csv\n",
    "with open('./Crawl_data/playlist_temp.csv','w',encoding=\"utf-8\",newline=\"\") as f:\n",
    "    writer = csv.writer(f)     \n",
    "    playlist_column = ['ID','Name','Link','UserName','Type','NumberTrack','Duration','DateCreate','Like','Repost','TrackIds']\n",
    "    writer.writerow(playlist_column)\n",
    "    for index in range(0,len(link_playlist)): \n",
    "        driver.get(url+link_playlist[index])   \n",
    "        try:\n",
    "            main =  WebDriverWait(driver,10).until(\n",
    "                EC.presence_of_element_located((By.CLASS_NAME,\"listenNetworkSidebar\"))\n",
    "            )\n",
    "        except:\n",
    "            print('TimeOut')\n",
    "            #neu bi time out thi bo qua link nay\n",
    "            continue\n",
    "        soup  = bs(driver.page_source, 'lxml')\n",
    "        #playlist_id\n",
    "        playlist_id = index\n",
    "\n",
    "        #playlist_name\n",
    "        playlist_name = soup.select('div.soundTitle__usernameTitleContainer h1 span')[0].get_text() \\\n",
    "            if soup.select('div.soundTitle__usernameTitleContainer h1 span') \\\n",
    "                else 'None'\n",
    "        #playlist_link\n",
    "        playlist_link=link_playlist[index]\n",
    "        #playlist_user\n",
    "        playlist_username = soup.select('div.soundTitle__usernameHeroContainer h2 a')[0].text.strip().strip('\\n') \\\n",
    "            if soup.select('div.soundTitle__usernameHeroContainer h2 a') \\\n",
    "                else 'None'\n",
    "        #playlist_type\n",
    "        playlist_type = soup.select('div.fullHero__foreground.fullListenHero__foreground div.fullHero__info a span')[0].text \\\n",
    "            if soup.select('div.fullHero__foreground.fullListenHero__foreground div.fullHero__info a span') \\\n",
    "                else 'None'\n",
    "        #playlist_numbertrack\n",
    "        playlist_numbertrack = '0'\n",
    "        #playlist_duration ( tong thoi gian cua cac track)\n",
    "        playlist_duration = '0'\n",
    "\n",
    "        #tracksSummary\n",
    "        tracksSummary = soup.select('div.fullHero__tracksSummary div div div') \n",
    "        if tracksSummary:   \n",
    "            playlist_numbertrack = soup.select('div.fullHero__tracksSummary div div div')[0].text  \n",
    "            if '0' == playlist_numbertrack:\n",
    "                #Xu ly o day boi vi co mot so track ma user an di, nhung van hien tong thoi gian duration mac gi so track public la 0\n",
    "                playlist_duration = '0'\n",
    "            else:\n",
    "                playlist_duration = soup.select('div.fullHero__tracksSummary div div div')[3].text     \n",
    "        #playlist_date\n",
    "        playlist_date = soup.select('div.fullHero__info div.fullHero__uploadTime.sc-type-medium.sc-text-body time')[0]['datetime'] \\\n",
    "            if soup.select('div.fullHero__info div.fullHero__uploadTime.sc-type-medium.sc-text-body time') \\\n",
    "                else '0'\n",
    "            \n",
    "        #playlist_ministats\n",
    "        playlist_like = '0'\n",
    "        playlist_repost = '0'\n",
    "        playlist_ministats = soup.select('div.listenEngagement__footer ul li')\n",
    "        for i in range(0,len(playlist_ministats)):\n",
    "            if 'like' in playlist_ministats[i]['title']:\n",
    "                playlist_like = playlist_ministats[i]['title'].split()[0]\n",
    "            elif 'repost' in playlist_ministats[i]['title']:\n",
    "                playlist_repost = playlist_ministats[i]['title'].split()[0]\n",
    "        \n",
    "        users = soup.select('div.trackItem__content.sc-truncate a.trackItem__username.sc-link-light.sc-link-secondary')\n",
    "        link_user = []\n",
    "        for i in range(0,len(users)):\n",
    "            link_user.append(url+ users[i]['href'])\n",
    "            #lay 4 user link dau tien xuat hien trong playlist neu so user >= 4\n",
    "            if 3==i:\n",
    "                break\n",
    "        link_alluser.extend(link_user)\n",
    "        #Xu ly duplicate bang pandas\n",
    "        link_alluser = pd.unique(link_alluser).tolist()\n",
    "\n",
    "        tracks = soup.select('div.trackItem__content.sc-truncate a.trackItem__trackTitle.sc-link-dark.sc-link-primary.sc-font-light')\n",
    "        playlist_trackIds = []\n",
    "        for i in range(0,len(tracks)):\n",
    "            temp_track = convert_track_link(tracks[i]['href'])\n",
    "            playlist_trackIds.append(url+ temp_track)\n",
    "            #lay 4 track link dau tien xuat hien trong playlist neu so track >= 4\n",
    "            if 3==i:\n",
    "                break\n",
    "        link_alltrack.extend(playlist_trackIds)\n",
    "        #Xu ly duplicate\n",
    "        link_alltrack = pd.unique(link_alltrack).tolist()\n",
    "        # playlist_track = []\n",
    "        #o day ghi truc tiep, nhung de phong trong qua trinh crawl ma bi loi timeout thi nen xy ly sau khi lay data xuong\n",
    "        # #ghi thong tin cua cot playlist_track/tracklds ( theo nhu de bai)\n",
    "        # for track in playlist_trackIds:\n",
    "        #     #o day co the bi sai neu track khong nam trong link_alltrack\n",
    "        #     #theo logic thi khong sai duoc\n",
    "        #     playlist_track.append(link_alltrack.index(track))\n",
    "        # #Xu ly 1 track co the duoc lap lai nhieu lan trong playlist\n",
    "        # playlist_track = pd.unique(playlist_track).tolist()\n",
    "        playlist_trackIds = ';'.join(str(v) for v in playlist_trackIds)\n",
    "    \n",
    "        row = [playlist_id,playlist_name,playlist_link,playlist_username,playlist_type,playlist_numbertrack,playlist_duration,playlist_date,playlist_like,playlist_repost,playlist_trackIds]\n",
    "        writer.writerow(row)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lưu thông tin track vào file `link_alluser.txt` và user vào `link_alltrack.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer1 = open('link_alluser.txt','wt',encoding=\"utf-8\")\n",
    "for i in link_alluser:\n",
    "    writer1.write(i)\n",
    "    writer1.write('\\n')\n",
    "writer1.close()\n",
    "writer2 = open('link_alltrack.txt','wt',encoding=\"utf-8\")\n",
    "for i in link_alltrack:\n",
    "    writer2.write(i)\n",
    "    writer2.write('\\n')\n",
    "writer2.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clear link_alltrack và link_alluser để lấy dữ liệu từ file `link_alltrack.txt` và `link_alluser.txt` sau khi đã xử lý phía trên"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clear list de dam bao khong con du lieu duoc luu trong list\n",
    "link_alltrack.clear()\n",
    "link_alluser.clear()\n",
    "#lay thong tin\n",
    "reader = open('link_alltrack.txt','r')\n",
    "for i in reader:\n",
    "    link_alltrack.append(i.strip('\\n'))\n",
    "reader.close()\n",
    "reader = open('link_alluser.txt','r')\n",
    "for i in reader:\n",
    "    link_alluser.append(i.strip('\\n'))\n",
    "reader.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract các links track bằng parse HTML và lưu thông tin vào `track_temp.csv`\n",
    "### File track_temp.csv gồm các cột sau:\n",
    "   - `ID`: số thứ tự của track tương tự như track do không lấy đươc id nên ta sẽ coi đây như id của user\n",
    "   - `Name`: tên của track\n",
    "   - `Link`: đường link tới track\n",
    "   - `UserName`: tên tác giả của track\n",
    "   - `Type`: loại track\n",
    "   - `Time`: thời lượng của track\n",
    "   - `Date`: thời gian tạo track\n",
    "   - `Played`: số lượng người nghe của track \n",
    "   - `Like`: số lượt like của track\n",
    "   - `Repost`: số repost của track\n",
    "   - Ngoài ra có một số trang có thể lấy được số comment nhưng do cấu trúc đường dẫn html khác và số lượng rất ít nên không có làm thêm trong bài (Những link này sẽ được loại bỏ sau khi crawl data xong)\n",
    "### Xử lý:\n",
    "   - Do chỉ cần tìm đúng tag chưa thông tin của cột nên ta chỉ cần dùng CSS Selector như bình thường\n",
    "   - Tương tự dùng WebDriverWait để đợi trang web load xong bằng cách tìm `class localeSelector`, class này nằm trong ô `language`, thường nằm ở cuối trang nên khi tìm được class này thì trang gần như load xong ( có thể có sai sót nhưng rất ít)\n",
    "   - `track_ministats` là list các tag `li` có chứa thông tin về lượt nghe, lượt like và lượt đăng lại nên ta sẽ lấy các thông tin trên dựa vào list này\n",
    "### Lưu ý:\n",
    "   - Do hiện [SoundCloud](https://soundcloud.com/) đang có nhiều kiểu trình bày trang track nên trong quá trình get, những trang được thiết kế theo kiểu mới, khi get data sẽ bị lỗi (chiếm số lượng ít hơn nhiều so với những track get được). Nguyên nhân nằm ở `track_ministats` (đã xử lý những trường hợp này sau khi crawl data xong)\n",
    "   - Cũng có thể do track này để chế độ private hoặc không cho phát ở khu vực Việt Nam nên quá trình get có thể bị lỗi, không cho ra thông tin về stats [ví dụ](https://soundcloud.com/atlantic-records-uk/grouplove-colours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xu li link_alltrack\n",
    "with open('./Crawl_data/track_temp.csv','w',encoding=\"utf-8\",newline=\"\") as f:\n",
    "    writer = csv.writer(f)    \n",
    "    track_column = ['ID','Name','Link','UserName','Type','Time','Date','Played','Like','Repost']\n",
    "    writer.writerow(track_column)\n",
    "    for index in range(0,len(link_alltrack)): #len(link_alltrack)\n",
    "        driver.get(link_alltrack[index])\n",
    "        try:\n",
    "            main =  WebDriverWait(driver,10).until(\n",
    "                EC.presence_of_element_located((By.CLASS_NAME,\"localeSelector\"))\n",
    "            )\n",
    "        except:\n",
    "            print('TimeOut')\n",
    "            continue\n",
    "        #   driver.quit()        \n",
    "        soup  = bs(driver.page_source, 'lxml')\n",
    "        #Track_ID\n",
    "        track_id = index\n",
    "        #Track_name\n",
    "        track_name = soup.select('div.soundTitle__usernameTitleContainer div.soundTitle__usernameHeroContainer h2 a')[0].text.strip().strip('\\n') \\\n",
    "            if soup.select('div.soundTitle__usernameTitleContainer div.soundTitle__usernameHeroContainer h2 a') \\\n",
    "                else 'None'\n",
    "        #track_link\n",
    "        track_link = link_alltrack[index]\n",
    "        #track_user\n",
    "        track_user = soup.select('div.soundTitle__titleContainer div.soundTitle__usernameTitleContainer h1 span')[0].text \\\n",
    "            if soup.select('div.soundTitle__titleContainer div.soundTitle__usernameTitleContainer h1 span') \\\n",
    "                else 'None'\n",
    "        #track_type\n",
    "        track_type = soup.select('div.fullHero__foreground.fullListenHero__foreground div.fullHero__info a')[0].text \\\n",
    "            if soup.select('div.fullHero__foreground.fullListenHero__foreground div.fullHero__info a') \\\n",
    "                else 'None'\n",
    "        #track_time\n",
    "        track_time = soup.select('div.playbackTimeline__duration span')[1].text \\\n",
    "            if soup.select('div.playbackTimeline__duration span') \\\n",
    "                else '0'\n",
    "        #track_date\n",
    "        track_date = soup.select('div.fullHero__info div.fullHero__uploadTime.sc-type-medium.sc-text-body time')[0]['datetime'] \\\n",
    "            if soup.select('div.fullHero__info div.fullHero__uploadTime.sc-type-medium.sc-text-body time') \\\n",
    "                else '0'\n",
    "        #track_ministats\n",
    "        track_played = '0'\n",
    "        track_like = '0'\n",
    "        track_repost = '0'\n",
    "        track_ministats = soup.select('div.listenEngagement__footer ul li')\n",
    "        #track_played\n",
    "        #track_like\n",
    "        #track_repost\n",
    "        for i in range(0,len(track_ministats)):\n",
    "            if  'play' in track_ministats[i]['title']:\n",
    "                track_played = track_ministats[i]['title'].split(' ')[0]\n",
    "            if  'like' in track_ministats[i]['title']:\n",
    "                track_like = track_ministats[i]['title'].split(' ')[0]\n",
    "            if  'repost' in track_ministats[i]['title']:\n",
    "                track_repost = track_ministats[i]['title'].split(' ')[0]\n",
    "        row = [track_id,track_name,track_link,track_user,track_type,track_time,track_date,track_played,track_like,track_repost]\n",
    "        writer.writerow(row)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract các links user bằng parse HTML và lưu thông tin vào `user1.csv`\n",
    "### File user1.csv gồm các cột sau:\n",
    "   - `ID`: số thứ tự của user, do không trích được id nên ta coi như đây là id của user.\n",
    "   - `DisplayName`: tên hiển thị của user\n",
    "   - `Verified`: Nếu tài khoản user này đã được `verified` thì sẽ có giá trị verifed còn không thì trả về `None`\n",
    "   - `Name/Country`: FullName và tên Quốc gia của user, nếu chỉ có 1 thuộc tính thì có thể hoặc đó là FullName hoặc đó là tên Quốc gia\n",
    "   - `CreaterBadge`: gói soundcloud mà user sử dụng\n",
    "   - `Link`: đường link tới user\n",
    "   - `Follower`: số người đang theo dõi user\n",
    "   - `Following`: số người mà user đang theo dõi\n",
    "   - `Tracks`: số bài hát của user\n",
    "   - `Facebook`: đường link tới Facebook của user\n",
    "   - `Instagram`: đường link tới Instagram của user\n",
    "   - `Twitter`: đường link tới Twitter của user\n",
    "### Xử lý:\n",
    "   - Tương tự dùng CSS Selector để lấy thông tin mỗi row\n",
    "   - `user_social` để lưu các tag có thông tin về mạng xã hội cá nhân\n",
    "   - Có thể lấy nhiều hơn link mạng xã hội cá nhân nhưng ở đây chỉ lấy 3 cái là Facebook, Instagram, Twitter vì được dùng phổ biến nhất\n",
    "   - `addition info` chứa các thẻ chưa thông tin về `Name/Country` và `CreateBadge`. Vì `addition info` có nhiều trường hợp nên ta phải giải quyết riêng\n",
    "   - Ở thông tin like, do có một tag chứa thông tin `Fans also like` giống với `like` nên ta phải xử lý thêm.\n",
    "   - Do khi lấy bị dính thêm text của thẻ span con `verified` nên ta cũng cần phải xử lý lại `DisplayName`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Xu ly link_alluser\n",
    "with open('./Crawl_data/user.csv','w',encoding=\"utf-8\",newline=\"\") as f:\n",
    "    writer = csv.writer(f)    \n",
    "    user_column = ['ID','DisplayName','Verified','Name/Country','CreaterBadge','Link','Follower','Following','Tracks','Like','Facebook','Instagram','Twitter']\n",
    "    writer.writerow(user_column)\n",
    "    for index in range(0,len(link_alluser)):\n",
    "        driver.get(link_alluser[index])        \n",
    "        try:\n",
    "            main =  WebDriverWait(driver,10).until(\n",
    "                EC.presence_of_element_located((By.CLASS_NAME,\"localeSelector\"))\n",
    "            )\n",
    "        except:\n",
    "            print('TimeOut')\n",
    "            # driver.quit()   \n",
    "            continue\n",
    "        soup  = bs(driver.page_source, 'lxml')\n",
    "        #user_stt\n",
    "        user_id = index\n",
    "        #user_displayname\n",
    "        user_displayname = soup.select('div.profileHeaderInfo.sc-media div.profileHeaderInfo__content.sc-media-content h2')[0].text.strip().strip('\\n').strip('Verified').strip().strip('\\n') \\\n",
    "            if soup.select('div.profileHeaderInfo.sc-media div.profileHeaderInfo__content.sc-media-content h2') \\\n",
    "                else 'None'\n",
    "        #user_verified\n",
    "        user_verified = soup.select('div.profileHeaderInfo.sc-media div.profileHeaderInfo__content.sc-media-content h2 div span span')[0].text \\\n",
    "            if soup.select('div.profileHeaderInfo.sc-media div.profileHeaderInfo__content.sc-media-content h2 div span span') \\\n",
    "                else 'None'\n",
    "        \n",
    "        #addition_info\n",
    "        #user_name_or_country    \n",
    "        user_name_or_country=''\n",
    "        #user_badge\n",
    "        user_badge=''\n",
    "        user_addition = soup.select('div.profileHeaderInfo.sc-media div.profileHeaderInfo__content.sc-media-content h3')\n",
    "        if len(user_addition) == 3:\n",
    "            user_name_or_country = user_addition[0].string + '/' +user_addition[1].string\n",
    "            user_badge = user_addition[2].text.strip().strip('\\n')\n",
    "        elif len(user_addition) == 2:\n",
    "            user_name_or_country = user_addition[0].string\n",
    "            user_badge = user_addition[1].text.strip().strip('\\n')\n",
    "        elif len(user_addition) == 1:\n",
    "            user_name_or_country = 'None'\n",
    "            user_badge = user_addition[0].text.strip().strip('\\n')\n",
    "        if not user_badge:\n",
    "            user_badge = 'None'\n",
    "        #user_link\n",
    "        user_link = link_alluser[index]\n",
    "        #user_follower\n",
    "        user_follower ='0'\n",
    "        #user_Following\n",
    "        user_following = '0'\n",
    "        #user_tracks\n",
    "        user_tracks = '0'\n",
    "        #user_stats\n",
    "        user_stats = soup.select('article.infoStats table tbody tr td a')\n",
    "        for i in range(0,len(user_stats)):\n",
    "            if 'follower' in  user_stats[i]['title']:\n",
    "                user_follower = user_stats[i]['title'].split()[0]\n",
    "            elif 'Following' in user_stats[i]['title']:\n",
    "                user_following = user_stats[i]['title'].split()[1]\n",
    "            elif 'track' in user_stats[i]['title']:\n",
    "                user_tracks = user_stats[i]['title'].split()[0]\n",
    "        #user_like\n",
    "        user_like ='0'\n",
    "        user_like_temp = soup.select('span.sidebarHeader__actualTitle.sc-text-h3') \\\n",
    "            if soup.select('span.sidebarHeader__actualTitle.sc-text-h3') \\\n",
    "                else 'None'\n",
    "        for like in user_like_temp:\n",
    "            if 'like' in like.text and 'Fans' not in like.text:\n",
    "                user_like = like.text.split()[0]\n",
    "        #user_facebook\n",
    "        user_facebook='None'\n",
    "        #user_instagram\n",
    "        user_instagram='None'\n",
    "        #user_twitter\n",
    "        user_twitter='None'\n",
    "        #user_social\n",
    "        user_social = soup.select('article.infoStats div.web-profiles ul li')\n",
    "        for i in range(0,len(user_social)):\n",
    "            if 'acebook' in user_social[i].div.a.text:\n",
    "                user_facebook = user_social[i].div.a['href']\n",
    "            if 'nstagram' in user_social[i].div.a.text:\n",
    "                user_instagram = user_social[i].div.a['href']\n",
    "            if 'witter' in user_social[i].div.a.text:\n",
    "                user_twitter = user_social[i].div.a['href']\n",
    "        row = [user_id,user_displayname,user_verified,user_name_or_country,user_badge,user_link,user_follower,user_following,user_tracks,user_like,user_facebook,user_instagram,user_twitter]\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mở 2 file playlist_temp và track_temp vào 2 DataFrame df1 và df2 bằng pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('./Crawl_data/playlist_temp.csv')\n",
    "df2 = pd.read_csv('./Crawl_data/track_temp.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Xử lý cho gọn file track_temp.csv:\n",
    "- Đầu tiên ta sẽ xoá những row có `Date = '0'`. Bằng cách tìm vị trí (index) có `Date = '0'`, sau đó dùng drop với `errors='ignore'`( phải set giá trị errors='ignore' loại bỏ hẳn các row bị drop)\n",
    "- Cập nhật lại index, sau đó cập nhật lại ID/STT theo index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loai nhung track ma khi lay xuong thong tin bi loi trong qua trinh get\n",
    "list_drop_track = []\n",
    "list_drop_track.clear()\n",
    "for i in range(0,len(df2)):\n",
    "    if df2.Date[i] == '0':\n",
    "        list_drop_track.append(i)\n",
    "\n",
    "for i in list_drop_track:\n",
    "    df2 = df2.drop(i,errors='ignore')\n",
    "#cap nhat lai index\n",
    "df2 = df2.reset_index(drop=True)\n",
    "# #chinh lai soID/STT cho dep\n",
    "for j in df2.index:\n",
    "    df2.at[j,'ID'] = j\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cập nhật lại TrackIds cho df1( playlist) và lưu file:\n",
    "- Bằng cách tìm ID của những link ở column `TrackIds` trong column `Link` của track\n",
    "- Lưu df1 và df2 sau khi được xử lý thành file Playlist.csv và Track.csv ( thực ra có thể thay đổi trực tiếp nhưng để đề phòng rủi ro trong quá trình xử lý data, tránh phải crawl lại data thì ta tách thành 2 file riêng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#do thay doi data tu thu muc goc nen cell nay chi duoc phep chay 1 lan, chay lan thu 2 se bi loi\n",
    "trackIDs = []\n",
    "link_track = df2['Link'].tolist()\n",
    "for i in range(0,len(df1['TrackIds'])):\n",
    "    if len(str(df1['TrackIds'][i])) == 0 :\n",
    "        df1.at[i,'TrackIds'] = ''\n",
    "    else:\n",
    "        temp4 = str(df1['TrackIds'][i]).split(';')\n",
    "        temp2 = df2['Link'].tolist()\n",
    "        temp1 =[]\n",
    "        for j in range(0,len(temp4)):\n",
    "            if temp4[j] in temp2:\n",
    "                temp1.append(str(temp2.index(temp4[j])))\n",
    "        temp3 = ';'.join(str(v) for v in temp1)\n",
    "        df1.at[i,'TrackIds'] = temp3\n",
    "       \n",
    "df1.to_csv('./Crawl_data/playlist.csv',index = False)\n",
    "df2.to_csv('./Crawl_data/track.csv',index = False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3226654afaf10d4725232a82851b81c5d4c00c81f3b9e3af252f5e3de70cd77b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
